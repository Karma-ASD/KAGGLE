{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/asdkarma/imc-2023-submission?scriptVersionId=138132878\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"d265148f","metadata":{"papermill":{"duration":0.012026,"end_time":"2023-07-28T03:30:26.727638","exception":false,"start_time":"2023-07-28T03:30:26.715612","status":"completed"},"tags":[]},"source":["## Submission\n","\n","A notebook to generate a valid submission. Implements local feature/matcher methods: LoFTR, DISK, and KeyNetAffNetHardNet, Swift, Akaze.\n"]},{"cell_type":"code","execution_count":1,"id":"d2931245","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:26.751503Z","iopub.status.busy":"2023-07-28T03:30:26.750615Z","iopub.status.idle":"2023-07-28T03:30:26.764293Z","shell.execute_reply":"2023-07-28T03:30:26.763281Z"},"papermill":{"duration":0.028165,"end_time":"2023-07-28T03:30:26.766595","exception":false,"start_time":"2023-07-28T03:30:26.73843","status":"completed"},"tags":[]},"outputs":[],"source":["SUBMIT = True\n","SET = \"test\" if SUBMIT else \"train\"\n","NUM_FEATS = 4096 if SUBMIT else 4096\n","RESIZE_SMALL_EDGE_TO=800\n","if SUBMIT:\n","    DESCRIPTOR = \"HARDNET\"\n","else:\n","    DESCRIPTOR = \"HARDNET\"\n","VERBOSE = False\n","COMPOSITE_DESCRIPTOR = True\n","UPRIGHT = False"]},{"cell_type":"code","execution_count":2,"id":"41393646","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-28T03:30:26.789581Z","iopub.status.busy":"2023-07-28T03:30:26.788607Z","iopub.status.idle":"2023-07-28T03:30:32.541049Z","shell.execute_reply":"2023-07-28T03:30:32.539873Z"},"papermill":{"duration":5.766917,"end_time":"2023-07-28T03:30:32.543953","exception":false,"start_time":"2023-07-28T03:30:26.777036","status":"completed"},"tags":[]},"outputs":[],"source":["# General utilities\n","import os\n","from tqdm import tqdm\n","from time import time\n","from fastprogress import progress_bar\n","import gc\n","import numpy as np\n","import pandas as pd\n","import h5py\n","from IPython.display import clear_output\n","from collections import defaultdict\n","from copy import deepcopy\n","\n","# CV/ML\n","import cv2\n","import torch\n","import torch.nn.functional as F\n","import kornia as K\n","import kornia.feature as KF\n","from PIL import Image\n","import timm\n","from timm.data import resolve_data_config\n","from timm.data.transforms_factory import create_transform\n","\n","# 3D reconstruction\n","import pycolmap\n","\n","\n","from imc_2023_eval import eval_submission\n","import pandas as pd\n","# import openglue"]},{"cell_type":"code","execution_count":3,"id":"edf5ff6c","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:32.56714Z","iopub.status.busy":"2023-07-28T03:30:32.566744Z","iopub.status.idle":"2023-07-28T03:30:32.571271Z","shell.execute_reply":"2023-07-28T03:30:32.570096Z"},"papermill":{"duration":0.019401,"end_time":"2023-07-28T03:30:32.574187","exception":false,"start_time":"2023-07-28T03:30:32.554786","status":"completed"},"tags":[]},"outputs":[],"source":["# !cd __notebook_source__.ipynb/kaggle/input/openglue/openglue\n","# !dir"]},{"cell_type":"code","execution_count":4,"id":"42c8a69b","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:32.596684Z","iopub.status.busy":"2023-07-28T03:30:32.59636Z","iopub.status.idle":"2023-07-28T03:30:34.103096Z","shell.execute_reply":"2023-07-28T03:30:34.101777Z"},"papermill":{"duration":1.521513,"end_time":"2023-07-28T03:30:34.106242","exception":false,"start_time":"2023-07-28T03:30:32.584729","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'OpenGlue'...\r\n","remote: Enumerating objects: 108, done.\u001b[K\r\n","remote: Counting objects: 100% (30/30), done.\u001b[K\r\n","remote: Compressing objects: 100% (20/20), done.\u001b[K\r\n","remote: Total 108 (delta 13), reused 10 (delta 10), pack-reused 78\u001b[K\r\n","Receiving objects: 100% (108/108), 324.11 KiB | 10.45 MiB/s, done.\r\n","Resolving deltas: 100% (24/24), done.\r\n"]}],"source":["!git clone https://github.com/ucuapps/OpenGlue.git"]},{"cell_type":"code","execution_count":5,"id":"acdfedb6","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:34.130856Z","iopub.status.busy":"2023-07-28T03:30:34.130458Z","iopub.status.idle":"2023-07-28T03:30:34.137789Z","shell.execute_reply":"2023-07-28T03:30:34.136687Z"},"papermill":{"duration":0.022294,"end_time":"2023-07-28T03:30:34.140098","exception":false,"start_time":"2023-07-28T03:30:34.117804","status":"completed"},"tags":[]},"outputs":[],"source":["import OpenGlue\n","from OpenGlue import models\n"]},{"cell_type":"code","execution_count":null,"id":"d02e28f0","metadata":{"papermill":{"duration":0.010849,"end_time":"2023-07-28T03:30:34.163549","exception":false,"start_time":"2023-07-28T03:30:34.1527","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"id":"c059ac57","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:34.188779Z","iopub.status.busy":"2023-07-28T03:30:34.188409Z","iopub.status.idle":"2023-07-28T03:30:34.254515Z","shell.execute_reply":"2023-07-28T03:30:34.253382Z"},"papermill":{"duration":0.081974,"end_time":"2023-07-28T03:30:34.256932","exception":false,"start_time":"2023-07-28T03:30:34.174958","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Kornia version 0.6.11\n","Pycolmap version 0.3.0\n"]}],"source":["print('Kornia version', K.__version__)\n","print('Pycolmap version', pycolmap.__version__)\n","\n","LOCAL_FEATURE = 'KeyNetAffNetHardNet'\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device=torch.device('cpu')\n","# Can be LoFTR, KeyNetAffNetHardNet, or DISK"]},{"cell_type":"code","execution_count":7,"id":"e0693354","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:34.281523Z","iopub.status.busy":"2023-07-28T03:30:34.280597Z","iopub.status.idle":"2023-07-28T03:30:34.287868Z","shell.execute_reply":"2023-07-28T03:30:34.286862Z"},"papermill":{"duration":0.021845,"end_time":"2023-07-28T03:30:34.290194","exception":false,"start_time":"2023-07-28T03:30:34.268349","status":"completed"},"tags":[]},"outputs":[],"source":["def arr_to_str(a):\n","    return ';'.join([str(x) for x in a.reshape(-1)])\n","\n","\n","def load_torch_image(fname, device=torch.device('cpu')):\n","    img = K.image_to_tensor(cv2.imread(fname), False).float() / 255.\n","    img = K.color.bgr_to_rgb(img.to(device))\n","    return img"]},{"cell_type":"code","execution_count":8,"id":"7ac130f1","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:34.314159Z","iopub.status.busy":"2023-07-28T03:30:34.313832Z","iopub.status.idle":"2023-07-28T03:30:34.332222Z","shell.execute_reply":"2023-07-28T03:30:34.331143Z"},"papermill":{"duration":0.033273,"end_time":"2023-07-28T03:30:34.334478","exception":false,"start_time":"2023-07-28T03:30:34.301205","status":"completed"},"tags":[]},"outputs":[],"source":["# We will use ViT global descriptor to get matching shortlists.\n","def get_global_desc(fnames, model,\n","                    device =  torch.device('cpu')):\n","    model = model.eval()\n","    model= model.to(device)\n","    config = resolve_data_config({}, model=model)\n","    transform = create_transform(**config)\n","    global_descs_convnext=[]\n","    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n","        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n","        img = Image.open(img_fname_full).convert('RGB')\n","        timg = transform(img).unsqueeze(0).to(device)\n","        with torch.no_grad():\n","            desc = model.forward_features(timg.to(device)).mean(dim=(-1,2))#\n","            #print (desc.shape)\n","            desc = desc.view(1, -1)\n","            desc_norm = F.normalize(desc, dim=1, p=2)\n","        #print (desc_norm)\n","        global_descs_convnext.append(desc_norm.detach().cpu())\n","    global_descs_all = torch.cat(global_descs_convnext, dim=0)\n","    return global_descs_all\n","\n","\n","def get_img_pairs_exhaustive(img_fnames):\n","    index_pairs = []\n","    for i in range(len(img_fnames)):\n","        for j in range(i+1, len(img_fnames)):\n","            index_pairs.append((i,j))\n","    return index_pairs\n","\n","\n","def get_image_pairs_shortlist(fnames,\n","                              sim_th = 0.6, # should be strict\n","                              min_pairs = 20,\n","                              exhaustive_if_less = 20,\n","                              device=torch.device('cpu')):\n","    num_imgs = len(fnames)\n","\n","    if num_imgs <= exhaustive_if_less:\n","        return get_img_pairs_exhaustive(fnames)\n","\n","    model = timm.create_model('tf_efficientnet_b7',\n","                              checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b7/1/tf_efficientnet_b7_ra-6c08e654.pth')\n","    model.eval()\n","    descs = get_global_desc(fnames, model, device=device)\n","    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n","    # removing half\n","    mask = dm <= sim_th\n","    total = 0\n","    matching_list = []\n","    ar = np.arange(num_imgs)\n","    already_there_set = []\n","    for st_idx in range(num_imgs-1):\n","        mask_idx = mask[st_idx]\n","        to_match = ar[mask_idx]\n","        if len(to_match) < min_pairs:\n","            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n","        for idx in to_match:\n","            if st_idx == idx:\n","                continue\n","            if dm[st_idx, idx] < 1000:\n","                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n","                total+=1\n","    matching_list = sorted(list(set(matching_list)))\n","    return matching_list"]},{"cell_type":"code","execution_count":9,"id":"cbdb2a93","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:34.358537Z","iopub.status.busy":"2023-07-28T03:30:34.358222Z","iopub.status.idle":"2023-07-28T03:30:34.393575Z","shell.execute_reply":"2023-07-28T03:30:34.392576Z"},"papermill":{"duration":0.050339,"end_time":"2023-07-28T03:30:34.396098","exception":false,"start_time":"2023-07-28T03:30:34.345759","status":"completed"},"tags":[]},"outputs":[],"source":["# Code to manipulate a colmap database.\n","# Forked from https://github.com/colmap/colmap/blob/dev/scripts/python/database.py\n","\n","# Copyright (c) 2018, ETH Zurich and UNC Chapel Hill.\n","# All rights reserved.\n","#\n","# Redistribution and use in source and binary forms, with or without\n","# modification, are permitted provided that the following conditions are met:\n","#\n","#     * Redistributions of source code must retain the above copyright\n","#       notice, this list of conditions and the following disclaimer.\n","#\n","#     * Redistributions in binary form must reproduce the above copyright\n","#       notice, this list of conditions and the following disclaimer in the\n","#       documentation and/or other materials provided with the distribution.\n","#\n","#     * Neither the name of ETH Zurich and UNC Chapel Hill nor the names of\n","#       its contributors may be used to endorse or promote products derived\n","#       from this software without specific prior written permission.\n","#\n","# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n","# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n","# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n","# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE\n","# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n","# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n","# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n","# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n","# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n","# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n","# POSSIBILITY OF SUCH DAMAGE.\n","#\n","# Author: Johannes L. Schoenberger (jsch-at-demuc-dot-de)\n","\n","# This script is based on an original implementation by True Price.\n","\n","import sys\n","import sqlite3\n","import numpy as np\n","\n","\n","IS_PYTHON3 = sys.version_info[0] >= 3\n","\n","MAX_IMAGE_ID = 2**31 - 1\n","\n","CREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n","    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n","    model INTEGER NOT NULL,\n","    width INTEGER NOT NULL,\n","    height INTEGER NOT NULL,\n","    params BLOB,\n","    prior_focal_length INTEGER NOT NULL)\"\"\"\n","\n","CREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n","    image_id INTEGER PRIMARY KEY NOT NULL,\n","    rows INTEGER NOT NULL,\n","    cols INTEGER NOT NULL,\n","    data BLOB,\n","    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\n","\n","CREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n","    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n","    name TEXT NOT NULL UNIQUE,\n","    camera_id INTEGER NOT NULL,\n","    prior_qw REAL,\n","    prior_qx REAL,\n","    prior_qy REAL,\n","    prior_qz REAL,\n","    prior_tx REAL,\n","    prior_ty REAL,\n","    prior_tz REAL,\n","    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n","    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n","\"\"\".format(MAX_IMAGE_ID)\n","\n","CREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\n","CREATE TABLE IF NOT EXISTS two_view_geometries (\n","    pair_id INTEGER PRIMARY KEY NOT NULL,\n","    rows INTEGER NOT NULL,\n","    cols INTEGER NOT NULL,\n","    data BLOB,\n","    config INTEGER NOT NULL,\n","    F BLOB,\n","    E BLOB,\n","    H BLOB)\n","\"\"\"\n","\n","CREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n","    image_id INTEGER PRIMARY KEY NOT NULL,\n","    rows INTEGER NOT NULL,\n","    cols INTEGER NOT NULL,\n","    data BLOB,\n","    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n","\"\"\"\n","\n","CREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n","    pair_id INTEGER PRIMARY KEY NOT NULL,\n","    rows INTEGER NOT NULL,\n","    cols INTEGER NOT NULL,\n","    data BLOB)\"\"\"\n","\n","CREATE_NAME_INDEX = \\\n","    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n","\n","CREATE_ALL = \"; \".join([\n","    CREATE_CAMERAS_TABLE,\n","    CREATE_IMAGES_TABLE,\n","    CREATE_KEYPOINTS_TABLE,\n","    CREATE_DESCRIPTORS_TABLE,\n","    CREATE_MATCHES_TABLE,\n","    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n","    CREATE_NAME_INDEX\n","])\n","\n","\n","def image_ids_to_pair_id(image_id1, image_id2):\n","    if image_id1 > image_id2:\n","        image_id1, image_id2 = image_id2, image_id1\n","    return image_id1 * MAX_IMAGE_ID + image_id2\n","\n","\n","def pair_id_to_image_ids(pair_id):\n","    image_id2 = pair_id % MAX_IMAGE_ID\n","    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n","    return image_id1, image_id2\n","\n","\n","def array_to_blob(array):\n","    if IS_PYTHON3:\n","        return array.tostring()\n","    else:\n","        return np.getbuffer(array)\n","\n","\n","def blob_to_array(blob, dtype, shape=(-1,)):\n","    if IS_PYTHON3:\n","        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n","    else:\n","        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\n","\n","\n","class COLMAPDatabase(sqlite3.Connection):\n","\n","    @staticmethod\n","    def connect(database_path):\n","        return sqlite3.connect(database_path, factory=COLMAPDatabase)\n","\n","\n","    def __init__(self, *args, **kwargs):\n","        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n","\n","        self.create_tables = lambda: self.executescript(CREATE_ALL)\n","        self.create_cameras_table = \\\n","            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n","        self.create_descriptors_table = \\\n","            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n","        self.create_images_table = \\\n","            lambda: self.executescript(CREATE_IMAGES_TABLE)\n","        self.create_two_view_geometries_table = \\\n","            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n","        self.create_keypoints_table = \\\n","            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n","        self.create_matches_table = \\\n","            lambda: self.executescript(CREATE_MATCHES_TABLE)\n","        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n","\n","    def add_camera(self, model, width, height, params,\n","                   prior_focal_length=False, camera_id=None):\n","        params = np.asarray(params, np.float64)\n","        cursor = self.execute(\n","            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n","            (camera_id, model, width, height, array_to_blob(params),\n","             prior_focal_length))\n","        return cursor.lastrowid\n","\n","    def add_image(self, name, camera_id,\n","                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n","        cursor = self.execute(\n","            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n","            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n","             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n","        return cursor.lastrowid\n","\n","    def add_keypoints(self, image_id, keypoints):\n","        assert(len(keypoints.shape) == 2)\n","        assert(keypoints.shape[1] in [2, 4, 6])\n","\n","        keypoints = np.asarray(keypoints, np.float32)\n","        self.execute(\n","            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n","            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n","\n","    def add_descriptors(self, image_id, descriptors):\n","        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n","        self.execute(\n","            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n","            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n","\n","    def add_matches(self, image_id1, image_id2, matches):\n","        assert(len(matches.shape) == 2)\n","        assert(matches.shape[1] == 2)\n","\n","        if image_id1 > image_id2:\n","            matches = matches[:,::-1]\n","\n","        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n","        matches = np.asarray(matches, np.uint32)\n","        self.execute(\n","            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n","            (pair_id,) + matches.shape + (array_to_blob(matches),))\n","\n","    def add_two_view_geometry(self, image_id1, image_id2, matches,\n","                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n","        assert(len(matches.shape) == 2)\n","        assert(matches.shape[1] == 2)\n","\n","        if image_id1 > image_id2:\n","            matches = matches[:,::-1]\n","\n","        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n","        matches = np.asarray(matches, np.uint32)\n","        F = np.asarray(F, dtype=np.float64)\n","        E = np.asarray(E, dtype=np.float64)\n","        H = np.asarray(H, dtype=np.float64)\n","        self.execute(\n","            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n","            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n","             array_to_blob(F), array_to_blob(E), array_to_blob(H)))"]},{"cell_type":"code","execution_count":10,"id":"ed72ed50","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:34.420315Z","iopub.status.busy":"2023-07-28T03:30:34.419987Z","iopub.status.idle":"2023-07-28T03:30:34.444139Z","shell.execute_reply":"2023-07-28T03:30:34.443081Z"},"papermill":{"duration":0.039543,"end_time":"2023-07-28T03:30:34.446955","exception":false,"start_time":"2023-07-28T03:30:34.407412","status":"completed"},"tags":[]},"outputs":[],"source":["# Code to interface DISK with Colmap.\n","# Forked from https://github.com/cvlab-epfl/disk/blob/37f1f7e971cea3055bb5ccfc4cf28bfd643fa339/colmap/h5_to_db.py\n","\n","#  Copyright [2020] [Michał Tyszkiewicz, Pascal Fua, Eduard Trulls]\n","#\n","#   Licensed under the Apache License, Version 2.0 (the \"License\");\n","#   you may not use this file except in compliance with the License.\n","#   You may obtain a copy of the License at\n","#\n","#       http://www.apache.org/licenses/LICENSE-2.0\n","#\n","#   Unless required by applicable law or agreed to in writing, software\n","#   distributed under the License is distributed on an \"AS IS\" BASIS,\n","#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","#   See the License for the specific language governing permissions and\n","#   limitations under the License.\n","\n","import os, argparse, h5py, warnings\n","import numpy as np\n","from tqdm import tqdm\n","from PIL import Image, ExifTags\n","\n","\n","def get_focal(image_path, err_on_default=False):\n","    image         = Image.open(image_path)\n","    max_size      = max(image.size)\n","\n","    exif = image.getexif()\n","    focal = None\n","    if exif is not None:\n","        focal_35mm = None\n","        # https://github.com/colmap/colmap/blob/d3a29e203ab69e91eda938d6e56e1c7339d62a99/src/util/bitmap.cc#L299\n","        for tag, value in exif.items():\n","            focal_35mm = None\n","            if ExifTags.TAGS.get(tag, None) == 'FocalLengthIn35mmFilm':\n","                focal_35mm = float(value)\n","                break\n","\n","        if focal_35mm is not None:\n","            focal = focal_35mm / 35. * max_size\n","    \n","    if focal is None:\n","        if err_on_default:\n","            raise RuntimeError(\"Failed to find focal length\")\n","\n","        # failed to find it in exif, use prior\n","        FOCAL_PRIOR = 1.2\n","        focal = FOCAL_PRIOR * max_size\n","\n","    return focal\n","\n","def create_camera(db, image_path, camera_model):\n","    image         = Image.open(image_path)\n","    width, height = image.size\n","\n","    focal = get_focal(image_path)\n","\n","    if camera_model == 'simple-pinhole':\n","        model = 0 # simple pinhole\n","        param_arr = np.array([focal, width / 2, height / 2])\n","    if camera_model == 'pinhole':\n","        model = 1 # pinhole\n","        param_arr = np.array([focal, focal, width / 2, height / 2])\n","    elif camera_model == 'simple-radial':\n","        model = 2 # simple radial\n","        param_arr = np.array([focal, width / 2, height / 2, 0.1])\n","    elif camera_model == 'opencv':\n","        model = 4 # opencv\n","        param_arr = np.array([focal, focal, width / 2, height / 2, 0., 0., 0., 0.])\n","         \n","    return db.add_camera(model, width, height, param_arr)\n","\n","\n","def add_keypoints(db, h5_path, image_path, img_ext, camera_model, single_camera = True):\n","    keypoint_f = h5py.File(os.path.join(h5_path, 'keypoints.h5'), 'r')\n","\n","    camera_id = None\n","    fname_to_id = {}\n","    for filename in tqdm(list(keypoint_f.keys())):\n","        keypoints = keypoint_f[filename][()]\n","\n","        fname_with_ext = filename# + img_ext\n","        path = os.path.join(image_path, fname_with_ext)\n","        if not os.path.isfile(path):\n","            raise IOError(f'Invalid image path {path}')\n","\n","        if camera_id is None or not single_camera:\n","            camera_id = create_camera(db, path, camera_model)\n","        image_id = db.add_image(fname_with_ext, camera_id)\n","        fname_to_id[filename] = image_id\n","\n","        db.add_keypoints(image_id, keypoints)\n","\n","    return fname_to_id\n","\n","def add_matches(db, h5_path, fname_to_id):\n","    match_file = h5py.File(os.path.join(h5_path, 'matches.h5'), 'r')\n","    \n","    added = set()\n","    n_keys = len(match_file.keys())\n","    n_total = (n_keys * (n_keys - 1)) // 2\n","\n","    with tqdm(total=n_total) as pbar:\n","        for key_1 in match_file.keys():\n","            group = match_file[key_1]\n","            for key_2 in group.keys():\n","                id_1 = fname_to_id[key_1]\n","                id_2 = fname_to_id[key_2]\n","\n","                pair_id = image_ids_to_pair_id(id_1, id_2)\n","                if pair_id in added:\n","                    warnings.warn(f'Pair {pair_id} ({id_1}, {id_2}) already added!')\n","                    continue\n","            \n","                matches = group[key_2][()]\n","                db.add_matches(id_1, id_2, matches)\n","\n","                added.add(pair_id)\n","\n","                pbar.update(1)"]},{"cell_type":"code","execution_count":11,"id":"5415229b","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:34.471949Z","iopub.status.busy":"2023-07-28T03:30:34.470845Z","iopub.status.idle":"2023-07-28T03:30:34.485132Z","shell.execute_reply":"2023-07-28T03:30:34.483967Z"},"papermill":{"duration":0.029312,"end_time":"2023-07-28T03:30:34.487544","exception":false,"start_time":"2023-07-28T03:30:34.458232","status":"completed"},"tags":[]},"outputs":[],"source":["# Making kornia local features loading w/o internet\n","class KeyNetAffNetHardNet(KF.LocalFeature):\n","    \"\"\"Convenience module, which implements KeyNet detector + AffNet + HardNet descriptor.\n","\n","    .. image:: _static/img/keynet_affnet.jpg\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        num_features: int = 5000,\n","        upright: bool = False,\n","        device = torch.device('cpu'),\n","        scale_laf: float = 1.0,\n","    ):\n","\n","        # ORI MODULE\n","        ori_module = KF.PassLAF() if upright else KF.LAFOrienter(angle_detector=KF.OriNet(False)).eval()\n","        if not upright:\n","            weights = torch.load('/kaggle/input/kornia-local-feature-weights/OriNet.pth')['state_dict']\n","            ori_module.angle_detector.load_state_dict(weights)\n","        \n","        # DETECTOR\n","        detector = KF.KeyNetDetector(\n","            False, num_features=num_features, ori_module=ori_module, aff_module=KF.LAFAffNetShapeEstimator(False).eval()\n","        ).to(device)\n","        kn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/keynet_pytorch.pth',map_location=device)['state_dict']\n","        detector.model.load_state_dict(kn_weights)\n","        \n","        affnet_weights = torch.load('/kaggle/input/kornia-local-feature-weights/AffNet.pth',map_location=device)['state_dict']\n","        detector.aff.load_state_dict(affnet_weights)\n","        \n","        # DESCRIPTOR\n","        if DESCRIPTOR==\"HARDNET\":\n","            patch_descriptor_module = KF.HardNet(False).eval()\n","            weights = torch.load('/kaggle/input/kornia-local-feature-weights/HardNetLib.pth',map_location=device)['state_dict']  \n","        elif DESCRIPTOR==\"HYNET\":\n","            patch_descriptor_module = KF.HyNet(False).eval()\n","            weights = torch.load('/kaggle/input/kornia-local-feature-weights/HyNet_LIB.pth',map_location=device)#['state_dict']\n","        elif DESCRIPTOR==\"SOSNET\":\n","            patch_descriptor_module = KF.SOSNet(False).eval()\n","            weights = torch.load('/kaggle/input/kornia-local-feature-weights/sosnet_32x32_liberty.pth',map_location=device)#['state_dict']\n","        patch_descriptor_module.load_state_dict(weights)\n","        descriptor = KF.LAFDescriptor(patch_descriptor_module, patch_size=32, grayscale_descriptor=True).to(device)\n","        super().__init__(detector, descriptor, scale_laf)\n"]},{"cell_type":"code","execution_count":12,"id":"c0352b32","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:34.511904Z","iopub.status.busy":"2023-07-28T03:30:34.510997Z","iopub.status.idle":"2023-07-28T03:30:37.435705Z","shell.execute_reply":"2023-07-28T03:30:37.43463Z"},"papermill":{"duration":2.939814,"end_time":"2023-07-28T03:30:37.438487","exception":false,"start_time":"2023-07-28T03:30:34.498673","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:24: DeprecationWarning: `LAFAffNetShapeEstimator` default behaviour is changed and now it does preserve original LAF orientation. Make sure your code accounts for this.\n"]}],"source":["DESCRIPTOR = \"HYNET\"\n","HYNET = KeyNetAffNetHardNet(4000, True, device).to(device).eval()\n","DESCRIPTOR = \"HARDNET\""]},{"cell_type":"code","execution_count":13,"id":"12d87b5d","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:37.464593Z","iopub.status.busy":"2023-07-28T03:30:37.463429Z","iopub.status.idle":"2023-07-28T03:30:37.470452Z","shell.execute_reply":"2023-07-28T03:30:37.4695Z"},"papermill":{"duration":0.02172,"end_time":"2023-07-28T03:30:37.47278","exception":false,"start_time":"2023-07-28T03:30:37.45106","status":"completed"},"tags":[]},"outputs":[],"source":["# img_fnames = [f'{src}/{SET}/{x}' for x in data_dict[\"haiper\"][\"bike\"]]\n","# # detect_features(img_fnames, \n","# #                                 NUM_FEATS,\n","# #                                 feature_dir= f'featureout/{\"haiper\"}_{\"bike\"}',\n","# #                                 upright=True,\n","# #                                 device=device,\n","# #                                 resize_small_edge_to=RESIZE_SMALL_EDGE_TO\n","# #                                )\n","# model_ = KF.SIFTFeature()\n","# x1,x2,y1,y2  = 500,750,250,500\n","# x1,x2,y1,y2  = 400,1200,200,700\n","\n","# # x1,x2,y1,y2  = 450,1500,250,1400\n","# crop = False\n","# if crop:\n","#     features = [disk]#[HARDNET1k]\n","# else:\n","#     features = [disk]#[HARDNET4k]\n","    \n","# for img_path in progress_bar(img_fnames[:1]):\n","#     img_fname = img_path.split('/')[-1]\n","#     with torch.inference_mode():\n","#         timg = load_torch_image(img_path, device=device)\n","#         if crop:\n","#             timg=timg[:,:,x1:x2,y1:y2]\n","#         H, W = timg.shape[2:]\n","#         print(H,W)\n","        \n","#         timg_resized = timg#K.geometry.resize(timg, None, antialias=True)\n","#         h, w = timg_resized.shape[2:]\n","#         begin=True\n","#         for feature in features:\n","#             _lafs, _resps, _descs = feature(K.color.rgb_to_grayscale(timg_resized))\n","#             if begin:\n","#                 lafs,resps,descs = _lafs, _resps, _descs\n","#                 begin = False\n","#             else:\n","#                 lafs, resps,descs = torch.cat((lafs,_lafs),1),torch.cat((resps,_resps),1),torch.cat((descs,_descs),1)\n","#         lafs[:,:,0,:] *= float(W) / float(w)\n","#         lafs[:,:,1,:] *= float(H) / float(h)\n","#         desc_dim = descs.shape[-1]\n","#         kpts = KF.get_laf_center(lafs).reshape(-1, 2).detach().cpu().numpy()\n","#         descs = descs.reshape(-1, desc_dim).detach().cpu().numpy()\n","# print(len(kpts))"]},{"cell_type":"code","execution_count":14,"id":"be8efd7a","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:37.498496Z","iopub.status.busy":"2023-07-28T03:30:37.49747Z","iopub.status.idle":"2023-07-28T03:30:37.50328Z","shell.execute_reply":"2023-07-28T03:30:37.502325Z"},"papermill":{"duration":0.020568,"end_time":"2023-07-28T03:30:37.505839","exception":false,"start_time":"2023-07-28T03:30:37.485271","status":"completed"},"tags":[]},"outputs":[],"source":["# import cv2\n","# import matplotlib.pyplot as plt\n","\n","# imageread = cv2.imread(img_path)\n","  \n","# # input image is converted to gray scale image\n","# imagegray = cv2.cvtColor(imageread, cv2.COLOR_BGR2GRAY)\n","# if crop:\n","#     imagegray = imagegray[x1:x2,y1:y2]\n","# pts = cv2.KeyPoint_convert(kpts)\n","\n","# # drawKeypoints function is used to draw keypoints\n","# output_image = cv2.drawKeypoints(imagegray, pts, 0, (255, 0, 0),\n","#                                  flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n","  \n","# # displaying the image with keypoints as the\n","# # output on the screen\n","# fig = plt.figure(figsize=(10,10))\n","# ax = fig.gca()\n","# ax.set_xticks(np.arange(0, W, 100))\n","# ax.set_yticks(np.arange(0, H, 100))\n","# plt.imshow(output_image[x1:x2,y1:y2])\n","# plt.grid(10)\n","# # plotting image\n","# plt.show()"]},{"cell_type":"code","execution_count":15,"id":"1ae8f7bd","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:37.529572Z","iopub.status.busy":"2023-07-28T03:30:37.529258Z","iopub.status.idle":"2023-07-28T03:30:37.533725Z","shell.execute_reply":"2023-07-28T03:30:37.532659Z"},"papermill":{"duration":0.019075,"end_time":"2023-07-28T03:30:37.536045","exception":false,"start_time":"2023-07-28T03:30:37.51697","status":"completed"},"tags":[]},"outputs":[],"source":["# data = kpts.astype(int)\n","# data = np.unique(data,axis=0)   \n","# len(data),len(data)/len(kpts)"]},{"cell_type":"code","execution_count":16,"id":"0af0566a","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:37.559972Z","iopub.status.busy":"2023-07-28T03:30:37.559616Z","iopub.status.idle":"2023-07-28T03:30:37.564111Z","shell.execute_reply":"2023-07-28T03:30:37.563055Z"},"papermill":{"duration":0.019263,"end_time":"2023-07-28T03:30:37.566681","exception":false,"start_time":"2023-07-28T03:30:37.547418","status":"completed"},"tags":[]},"outputs":[],"source":["# from sklearn.cluster import DBSCAN\n","# from scipy.spatial.distance import cdist\n","# clustering = DBSCAN(eps=4, min_samples=4).fit(data)\n","# np.unique(clustering.labels_)"]},{"cell_type":"code","execution_count":17,"id":"3f680b6e","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:37.590564Z","iopub.status.busy":"2023-07-28T03:30:37.590222Z","iopub.status.idle":"2023-07-28T03:30:37.595294Z","shell.execute_reply":"2023-07-28T03:30:37.594226Z"},"papermill":{"duration":0.019967,"end_time":"2023-07-28T03:30:37.597622","exception":false,"start_time":"2023-07-28T03:30:37.577655","status":"completed"},"tags":[]},"outputs":[],"source":["# dbscan_res = {}\n","# wcss = {}\n","# core_indices = clustering.core_sample_indices_\n","# labels = clustering.labels_\n","# for idx in range(len(data)):\n","#     label = labels[idx]\n","#     if label!=-1:\n","#         wcss.setdefault(label,{\"count\":0,\"dist\":0})\n","#         wcss[label][\"count\"]+=1\n","#         try:\n","#             wcss[label][\"dist\"]+=min(cdist(data[core_indices],[data[idx]]))[0]\n","#         except:\n","#             continue\n","# for label in wcss:\n","#     wcss[label][\"avg\"]=wcss[label][\"dist\"]/(wcss[label][\"count\"]+1)\n","# wcss"]},{"cell_type":"code","execution_count":18,"id":"fa9b9dd1","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:37.621997Z","iopub.status.busy":"2023-07-28T03:30:37.62166Z","iopub.status.idle":"2023-07-28T03:30:37.690274Z","shell.execute_reply":"2023-07-28T03:30:37.689231Z"},"papermill":{"duration":0.083811,"end_time":"2023-07-28T03:30:37.692841","exception":false,"start_time":"2023-07-28T03:30:37.60903","status":"completed"},"tags":[]},"outputs":[],"source":["def detect_features(img_fnames,\n","                    num_feats = 2048,\n","                    upright = False,\n","                    device=torch.device('cpu'),\n","                    feature_dir = '.featureout',\n","                    resize_small_edge_to = 600):\n","#     if LOCAL_FEATURE == 'DISK':\n","        # Load DISK from Kaggle models so it can run when the notebook is offline.\n","    disk = KF.DISK().to(device)\n","    pretrained_dict = torch.load('/kaggle/input/disk/pytorch/depth-supervision/1/loftr_outdoor.ckpt', map_location=device)\n","    disk.load_state_dict(pretrained_dict['extractor'])\n","    disk.eval()\n","    HARDNET = KeyNetAffNetHardNet(num_feats, upright, device,).to(device)\n","    HARDNET.eval()\n","    SP=torch.load()\n","    \n","    if LOCAL_FEATURE == 'KeyNetAffNetHardNet':\n","        \n","        if COMPOSITE_DESCRIPTOR:\n","            features_detectors = {\n","                \"DISK\":disk,\n","                \"KAHN\":HARDNET,\n","                \"HYNET\":HYNET,\n","    #             \"sift\":KF.SIFTFeature(num_features=num_feats)\n","            \n","            }\n","        else:\n","            features_detectors = [KeyNetAffNetHardNet(num_feats, upright, device).to(device).eval()]\n","    if not os.path.isdir(feature_dir):\n","        os.makedirs(feature_dir)\n","    with h5py.File(f'{feature_dir}/lafs.h5', mode='w') as f_laf, \\\n","         h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n","         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n","        for img_path in progress_bar(img_fnames):\n","            img_fname = img_path.split('/')[-1]\n","            key = img_fname\n","            with torch.inference_mode():\n","                timg = load_torch_image(img_path, device=device)\n","                H, W = timg.shape[2:]\n","                if resize_small_edge_to is None:\n","                    timg_resized = timg\n","                else:\n","                    timg_resized = K.geometry.resize(timg, resize_small_edge_to, antialias=True)\n","                h, w = timg_resized.shape[2:]\n","#                 if LOCAL_FEATURE == 'DISK':\n","#                     features = disk(timg_resized, num_feats, pad_if_not_divisible=True)[0]\n","#                     kps1, descs = features.keypoints, features.descriptors\n","#                     lafs = KF.laf_from_center_scale_ori(kps1[None], torch.ones(1, len(kps1), 1, 1, device=device))\n","                    \n","                if LOCAL_FEATURE == 'KeyNetAffNetHardNet':\n","                    begin = True\n","                    try:\n","                        for feature_name,feature in features_detectors.items():\n","                            if feature_name==\"DISK\":\n","                                feats_ = feature(timg_resized, num_feats, pad_if_not_divisible=True)[0]\n","                                _kps, _descs = feats_.keypoints, torch.unsqueeze(feats_.descriptors,0)\n","                                _lafs = KF.laf_from_center_scale_ori(_kps[None], torch.ones(1, len(_kps), 1, 1, device=device))\n","                            else:\n","                                _lafs, _, _descs = feature(K.color.rgb_to_grayscale(timg_resized))\n","                            if begin:\n","                                lafs,descs = _lafs, _descs\n","                                begin = False\n","                            else:\n","                                \n","                                lafs,descs = torch.cat((lafs,_lafs),1),torch.cat((descs,_descs),1)\n","                    except Exception as e:\n","                        print(\"ERORORORORORORORO\",e)\n","                                    \n","                lafs[:,:,0,:] *= float(W) / float(w)\n","                lafs[:,:,1,:] *= float(H) / float(h)\n","                desc_dim = descs.shape[-1]\n","                kpts = KF.get_laf_center(lafs).reshape(-1, 2).detach().cpu().numpy()\n","                descs = descs.reshape(-1, desc_dim).detach().cpu().numpy()\n","                f_laf[key] = lafs.detach().cpu().numpy()\n","                f_kp[key] = kpts\n","                f_desc[key] = descs\n","    return\n","\n","def get_unique_idxs(A, dim=0):\n","    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n","    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n","    _, ind_sorted = torch.sort(idx, stable=True)\n","    cum_sum = counts.cumsum(0)\n","    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n","    first_indices = ind_sorted[cum_sum]\n","    return first_indices\n","\n","def match_features(img_fnames,\n","                   index_pairs,\n","                   feature_dir = '.featureout',\n","                   device=torch.device('cpu'),\n","                   min_matches=15, \n","                   force_mutual = True,\n","                   matching_alg='smnn'\n","                  ):\n","    assert matching_alg in ['smnn', 'adalam']\n","    with h5py.File(f'{feature_dir}/lafs.h5', mode='r') as f_laf, \\\n","         h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n","        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n","\n","        for pair_idx in progress_bar(index_pairs):\n","            idx1, idx2 = pair_idx\n","            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n","            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n","            lafs1 = torch.from_numpy(f_laf[key1][...]).to(device)\n","            lafs2 = torch.from_numpy(f_laf[key2][...]).to(device)\n","            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n","            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n","            if matching_alg == 'adalam':\n","                img1, img2 = cv2.imread(fname1), cv2.imread(fname2)\n","                hw1, hw2 = img1.shape[:2], img2.shape[:2]\n","                adalam_config = KF.adalam.get_adalam_default_config()\n","                #adalam_config['orientation_difference_threshold'] = None\n","                #adalam_config['scale_rate_threshold'] = None\n","                adalam_config['force_seed_mnn']= False\n","                adalam_config['search_expansion'] = 16\n","                adalam_config['ransac_iters'] = 128\n","                adalam_config['device'] = device\n","                dists, idxs = KF.match_adalam(desc1, desc2,\n","                                              lafs1, lafs2, # Adalam takes into account also geometric information\n","                                              hw1=hw1, hw2=hw2,\n","                                              config=adalam_config) # Adalam also benefits from knowing image size\n","            else:\n","                dists, idxs = KF.match_smnn(desc1, desc2, 0.98)\n","            if len(idxs)  == 0:\n","                continue\n","            # Force mutual nearest neighbors\n","            if force_mutual:\n","                first_indices = get_unique_idxs(idxs[:,1])\n","                idxs = idxs[first_indices]\n","                dists = dists[first_indices]\n","            n_matches = len(idxs)\n","            if False:\n","                print (f'{key1}-{key2}: {n_matches} matches')\n","            group  = f_match.require_group(key1)\n","            if n_matches >= min_matches:\n","                 group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n","    return\n","\n","def match_loftr(img_fnames,\n","                   index_pairs,\n","                   feature_dir = '.featureout_loftr',\n","                   device=torch.device('cpu'),\n","                   min_matches=15, resize_to_ = (640, 480)):\n","    matcher = KF.LoFTR(pretrained=None)\n","    matcher.load_state_dict(torch.load('/kaggle/input/loftr/pytorch/outdoor/1/loftr_outdoor.ckpt')['state_dict'])\n","    matcher = matcher.to(device).eval()\n","\n","    # First we do pairwise matching, and then extract \"keypoints\" from loftr matches.\n","    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='w') as f_match:\n","        for pair_idx in progress_bar(index_pairs):\n","            idx1, idx2 = pair_idx\n","            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n","            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n","            # Load img1\n","            timg1 = K.color.rgb_to_grayscale(load_torch_image(fname1, device=device))\n","            H1, W1 = timg1.shape[2:]\n","            if H1 < W1:\n","                resize_to = resize_to_[1], resize_to_[0]\n","            else:\n","                resize_to = resize_to_\n","            timg_resized1 = K.geometry.resize(timg1, resize_to, antialias=True)\n","            h1, w1 = timg_resized1.shape[2:]\n","\n","            # Load img2\n","            timg2 = K.color.rgb_to_grayscale(load_torch_image(fname2, device=device))\n","            H2, W2 = timg2.shape[2:]\n","            if H2 < W2:\n","                resize_to2 = resize_to[1], resize_to[0]\n","            else:\n","                resize_to2 = resize_to_\n","            timg_resized2 = K.geometry.resize(timg2, resize_to2, antialias=True)\n","            h2, w2 = timg_resized2.shape[2:]\n","            with torch.inference_mode():\n","                input_dict = {\"image0\": timg_resized1,\"image1\": timg_resized2}\n","                correspondences = matcher(input_dict)\n","            mkpts0 = correspondences['keypoints0'].cpu().numpy()\n","            mkpts1 = correspondences['keypoints1'].cpu().numpy()\n","\n","            mkpts0[:,0] *= float(W1) / float(w1)\n","            mkpts0[:,1] *= float(H1) / float(h1)\n","\n","            mkpts1[:,0] *= float(W2) / float(w2)\n","            mkpts1[:,1] *= float(H2) / float(h2)\n","\n","            n_matches = len(mkpts1)\n","            group  = f_match.require_group(key1)\n","            if n_matches >= min_matches:\n","                 group.create_dataset(key2, data=np.concatenate([mkpts0, mkpts1], axis=1))\n","\n","    # Let's find unique loftr pixels and group them together.\n","    kpts = defaultdict(list)\n","    match_indexes = defaultdict(dict)\n","    total_kpts=defaultdict(int)\n","    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='r') as f_match:\n","        for k1 in f_match.keys():\n","            group  = f_match[k1]\n","            for k2 in group.keys():\n","                matches = group[k2][...]\n","                total_kpts[k1]\n","                kpts[k1].append(matches[:, :2])\n","                kpts[k2].append(matches[:, 2:])\n","                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n","                current_match[:, 0]+=total_kpts[k1]\n","                current_match[:, 1]+=total_kpts[k2]\n","                total_kpts[k1]+=len(matches)\n","                total_kpts[k2]+=len(matches)\n","                match_indexes[k1][k2]=current_match\n","\n","    for k in kpts.keys():\n","        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n","    unique_kpts = {}\n","    unique_match_idxs = {}\n","    out_match = defaultdict(dict)\n","    for k in kpts.keys():\n","        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n","        unique_match_idxs[k] = uniq_reverse_idxs\n","        unique_kpts[k] = uniq_kps.numpy()\n","    for k1, group in match_indexes.items():\n","        for k2, m in group.items():\n","            m2 = deepcopy(m)\n","            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n","            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n","            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n","                                    unique_kpts[k2][  m2[:,1]],\n","                                   ],\n","                                   axis=1)\n","            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n","            m2_semiclean = m2[unique_idxs_current]\n","            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n","            m2_semiclean = m2_semiclean[unique_idxs_current1]\n","            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n","            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n","            out_match[k1][k2] = m2_semiclean2.numpy()\n","    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n","        for k, kpts1 in unique_kpts.items():\n","            f_kp[k] = kpts1\n","    \n","    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n","        for k1, gr in out_match.items():\n","            group  = f_match.require_group(k1)\n","            for k2, match in gr.items():\n","                group[k2] = match\n","    return\n","\n","def import_into_colmap(img_dir,\n","                       feature_dir ='.featureout',\n","                       database_path = 'colmap.db',\n","                       img_ext='.jpg'):\n","    db = COLMAPDatabase.connect(database_path)\n","    db.create_tables()\n","    single_camera = False\n","    fname_to_id = add_keypoints(db, feature_dir, img_dir, img_ext, 'simple-radial', single_camera)\n","    add_matches(\n","        db,\n","        feature_dir,\n","        fname_to_id,\n","    )\n","\n","    db.commit()\n","    return"]},{"cell_type":"code","execution_count":19,"id":"878fa7d9","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:37.717861Z","iopub.status.busy":"2023-07-28T03:30:37.716868Z","iopub.status.idle":"2023-07-28T03:30:37.722099Z","shell.execute_reply":"2023-07-28T03:30:37.721069Z"},"papermill":{"duration":0.020148,"end_time":"2023-07-28T03:30:37.724492","exception":false,"start_time":"2023-07-28T03:30:37.704344","status":"completed"},"tags":[]},"outputs":[],"source":["src = '/kaggle/input/image-matching-challenge-2023'"]},{"cell_type":"code","execution_count":20,"id":"7312deed","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:37.748652Z","iopub.status.busy":"2023-07-28T03:30:37.748287Z","iopub.status.idle":"2023-07-28T03:30:37.774052Z","shell.execute_reply":"2023-07-28T03:30:37.772671Z"},"papermill":{"duration":0.040793,"end_time":"2023-07-28T03:30:37.776455","exception":false,"start_time":"2023-07-28T03:30:37.735662","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["2cfa01ab573141e4 / 2fa124afd1f74f38 -> 3 images\n"]}],"source":["# Get data from csv.\n","\n","data_dict = {}\n","if SUBMIT:\n","    sample_submission_file = f'{src}/sample_submission.csv'\n","else:\n","    sample_submission_file = f'{src}/train/train_labels.csv'\n","    \n","sample_df = pd.read_csv(sample_submission_file)\n","\n","\n","for i,j in sample_df.iterrows():\n","    dataset = j[\"dataset\"]\n","    scene = j[\"scene\"]\n","    \n","    data_dict.setdefault(dataset,{}).setdefault(scene,[]).append(j[\"image_path\"])\n","\n","if False:\n","    for dataset,value in data_dict.items():\n","        for scene,image_dir in value.items():\n","            scene_dir = f\"{src}/{SET}/{dataset}/{scene}\"\n","            if \"images_full\" in os.listdir(scene_dir):\n","\n","                image_dir = f\"{scene_dir}/images_full\"\n","                for image in os.listdir(image_dir):\n","                    data_dict[dataset][scene].append(os.path.join(image_dir,image))\n","\n","if not SUBMIT:\n","    a = data_dict[\"heritage\"].pop(\"dioscuri\")\n","#     b = data_dict.pop(\"haiper\")\n","for dataset in data_dict:\n","    for scene in data_dict[dataset]:\n","        print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')"]},{"cell_type":"code","execution_count":21,"id":"953bab4f","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:37.801026Z","iopub.status.busy":"2023-07-28T03:30:37.800681Z","iopub.status.idle":"2023-07-28T03:30:37.825134Z","shell.execute_reply":"2023-07-28T03:30:37.823882Z"},"papermill":{"duration":0.039401,"end_time":"2023-07-28T03:30:37.827586","exception":false,"start_time":"2023-07-28T03:30:37.788185","status":"completed"},"tags":[]},"outputs":[],"source":["def main_func():\n","    gc.collect()\n","    datasets = []\n","    for dataset in data_dict:\n","        datasets.append(dataset)\n","\n","    for dataset in datasets:\n","        print(dataset)\n","        if dataset not in out_results:\n","            out_results[dataset] = {}\n","        for scene in data_dict[dataset]:\n","            print(scene)\n","            # Fail gently if the notebook has not been submitted and the test data is not populated.\n","            # You may want to run this on the training data in that case?\n","            img_dir = f'{src}/{SET}/{dataset}/{scene}/images'\n","            if not os.path.exists(img_dir):\n","                continue\n","            # Wrap the meaty part in a try-except block.\n","            try:\n","                out_results[dataset][scene] = {}\n","                img_fnames = [f'{src}/{SET}/{x}' for x in data_dict[dataset][scene]]\n","                print (f\"Got {len(img_fnames)} images\")\n","                feature_dir = f'featureout/{dataset}_{scene}'\n","                if not os.path.isdir(feature_dir):\n","                    os.makedirs(feature_dir, exist_ok=True)\n","                t=time()\n","                index_pairs = get_image_pairs_shortlist(img_fnames,\n","                                      sim_th = 0.5, # should be strict\n","                                      min_pairs = 20, # we select at least min_pairs PER IMAGE with biggest similarity\n","                                      exhaustive_if_less = 20,\n","                                      device=device)\n","                \n","                t=time() -t \n","                timings['shortlisting'].append(t)\n","                print (f'{len(index_pairs)}, pairs to match, {t:.4f} sec')\n","                gc.collect()\n","                t=time()\n","                if LOCAL_FEATURE != 'LoFTR':\n","                    detect_features(img_fnames, \n","                                NUM_FEATS,\n","                                feature_dir=feature_dir,\n","                                upright=UPRIGHT,\n","                                device=device,\n","                                resize_small_edge_to=RESIZE_SMALL_EDGE_TO\n","                               )\n","                    gc.collect()\n","                    t=time() -t \n","                    timings['feature_detection'].append(t)\n","                    print(f'Features detected in  {t:.4f} sec')\n","                    t=time()\n","                    match_features(img_fnames, index_pairs, feature_dir=feature_dir,device=device,matching_alg='smnn')\n","                else:\n","                    match_loftr(img_fnames, index_pairs, feature_dir=feature_dir, device=device, resize_to_=(600, 800))\n","                t=time() -t \n","                timings['feature_matching'].append(t)\n","                print(f'Features matched in  {t:.4f} sec')\n","                database_path = f'{feature_dir}/colmap.db'\n","                if os.path.isfile(database_path):\n","                    os.remove(database_path)\n","                gc.collect()\n","                import_into_colmap(img_dir, feature_dir=feature_dir,database_path=database_path)\n","                output_path = f'{feature_dir}/colmap_rec_{LOCAL_FEATURE}'\n","\n","                t=time()\n","                pycolmap.match_exhaustive(database_path,verbose=VERBOSE)\n","                t=time() - t \n","                timings['RANSAC'].append(t)\n","                print(f'RANSAC in  {t:.4f} sec')\n","\n","                t=time()\n","                # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n","                mapper_options = pycolmap.IncrementalMapperOptions()\n","                mapper_options.min_model_size = 3\n","                os.makedirs(output_path, exist_ok=True)\n","                maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, output_path=output_path, options=mapper_options,)\n","                print(maps)\n","                #clear_output(wait=False)\n","                t=time() - t\n","                timings['Reconstruction'].append(t)\n","                print(f'Reconstruction done in  {t:.4f} sec')\n","                imgs_registered  = 0\n","                best_idx = None\n","#                 print (\"Looking for the best reconstruction\")\n","                if isinstance(maps, dict):\n","                    for idx1, rec in maps.items():\n","#                         print (idx1, rec.summary())\n","                        if len(rec.images) > imgs_registered:\n","                            imgs_registered = len(rec.images)\n","                            best_idx = idx1\n","                if best_idx is not None:\n","#                     print (maps[best_idx].summary())\n","                    for k, im in maps[best_idx].images.items():\n","                        key1 = f'{dataset}/{scene}/images/{im.name}'\n","                        out_results[dataset][scene][key1] = {}\n","                        out_results[dataset][scene][key1][\"R\"] = deepcopy(im.rotmat())\n","                        out_results[dataset][scene][key1][\"t\"] = deepcopy(np.array(im.tvec))\n","                print(f'Registered: {dataset} / {scene} -> {len(out_results[dataset][scene])} images')\n","                print(f'Total: {dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n","                create_submission(out_results, data_dict)\n","                gc.collect()\n","            except:\n","                pass"]},{"cell_type":"code","execution_count":22,"id":"e285fe61","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:37.851437Z","iopub.status.busy":"2023-07-28T03:30:37.851112Z","iopub.status.idle":"2023-07-28T03:30:37.856369Z","shell.execute_reply":"2023-07-28T03:30:37.855287Z"},"papermill":{"duration":0.020025,"end_time":"2023-07-28T03:30:37.858705","exception":false,"start_time":"2023-07-28T03:30:37.83868","status":"completed"},"tags":[]},"outputs":[],"source":["def assert_func(s:str)->bool:\n","    s = s.lower()\n","    if \"n\" in s or \";;\" in s:\n","        return False\n","    return True"]},{"cell_type":"code","execution_count":23,"id":"d3e64508","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:37.883527Z","iopub.status.busy":"2023-07-28T03:30:37.882502Z","iopub.status.idle":"2023-07-28T03:30:37.895106Z","shell.execute_reply":"2023-07-28T03:30:37.89401Z"},"papermill":{"duration":0.027615,"end_time":"2023-07-28T03:30:37.897568","exception":false,"start_time":"2023-07-28T03:30:37.869953","status":"completed"},"tags":[]},"outputs":[],"source":["# Function to create a submission file.\n","def create_submission(data_dict):\n","    \n","    main_func()\n","    with open(f'submission.csv', 'w') as f:\n","        f.write('image_path,dataset,scene,rotation_matrix,translation_vector\\n')\n","        for dataset in data_dict:\n","            if dataset in out_results:\n","                res = out_results[dataset]\n","            else:\n","                res = {}\n","            for scene in data_dict[dataset]:\n","                if scene in res:\n","                    scene_res = res[scene]\n","                else:\n","                    scene_res = {\"R\":{}, \"t\":{}}\n","                for image in data_dict[dataset][scene]:\n","                    if \"full\" not in image:\n","                        try:\n","                            if image in scene_res:\n","                                print (image)\n","                                R = scene_res[image]['R'].reshape(-1)\n","                                T = scene_res[image]['t'].reshape(-1)\n","                            else:\n","                                R = np.eye(3).reshape(-1)\n","                                T = np.zeros((3))\n","\n","                            s1 = arr_to_str(R)\n","                            s2 = arr_to_str(T)\n","                            assert assert_func(s1) and assert_func(s2)\n","\n","                        except:\n","                            R = np.eye(3).reshape(-1)\n","                            T = np.zeros((3))\n","                        f.write(f'{image},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n')"]},{"cell_type":"code","execution_count":24,"id":"49830cfb","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:37.925503Z","iopub.status.busy":"2023-07-28T03:30:37.925089Z","iopub.status.idle":"2023-07-28T03:30:38.127097Z","shell.execute_reply":"2023-07-28T03:30:38.126051Z"},"papermill":{"duration":0.216971,"end_time":"2023-07-28T03:30:38.129453","exception":false,"start_time":"2023-07-28T03:30:37.912482","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["2cfa01ab573141e4\n","2fa124afd1f74f38\n"]}],"source":["out_results = {}\n","timings = {\"shortlisting\":[],\n","               \"feature_detection\": [],\n","               \"feature_matching\":[],\n","               \"RANSAC\": [],\n","               \"Reconstruction\": []}\n","if SUBMIT:\n","    create_submission(data_dict)"]},{"cell_type":"code","execution_count":25,"id":"b227b5ae","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:38.153917Z","iopub.status.busy":"2023-07-28T03:30:38.153545Z","iopub.status.idle":"2023-07-28T03:30:38.159077Z","shell.execute_reply":"2023-07-28T03:30:38.157956Z"},"papermill":{"duration":0.02086,"end_time":"2023-07-28T03:30:38.161651","exception":false,"start_time":"2023-07-28T03:30:38.140791","status":"completed"},"tags":[]},"outputs":[],"source":["\n","# if not SUBMIT:        \n","#     rotation_thresholds_degrees_dict = {\n","#         **{('haiper', scene): np.linspace(0.2, 10, 10) for scene in ['bike', 'chairs', 'fountain']},\n","#         **{('heritage', scene): np.linspace(0.2, 10, 10) for scene in ['cyprus', 'dioscuri']},\n","#         **{('heritage', 'wall'): np.linspace(0.2, 10, 10)},\n","#         **{('urban', 'kyiv-puppet-theater'): np.linspace(0.2, 10, 10)},\n","#     }\n","\n","#     translation_thresholds_meters_dict = {\n","#         **{('haiper', scene): np.geomspace(0.05, 1, 10) for scene in ['bike', 'chairs', 'fountain']},\n","#         **{('heritage', scene): np.geomspace(0.05, 1, 10) for scene in ['cyprus', 'dioscuri']},\n","#         **{('heritage', 'wall'): np.geomspace(0.05, 1, 10)},\n","#         **{('urban', 'kyiv-puppet-theater'): np.geomspace(0.5, 1, 10)},\n","#     }\n","    \n","#     df = pd.read_csv(\"/kaggle/input/image-matching-challenge-2023/train/train_labels.csv\")\n","#     df = df[['image_path','dataset', 'scene',  'rotation_matrix',\n","#            'translation_vector']]\n","#     df = df[df[\"scene\"]!=\"dioscuri\"]\n","#     df.to_csv(\"train.csv\",index=False)\n","    \n","#     eval_submission(submission_csv_path='submission.csv',\n","#                ground_truth_csv_path='train.csv',\n","#                rotation_thresholds_degrees_dict=rotation_thresholds_degrees_dict,\n","#                translation_thresholds_meters_dict=translation_thresholds_meters_dict,\n","#                verbose=True)"]},{"cell_type":"code","execution_count":26,"id":"500923c1","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:38.186Z","iopub.status.busy":"2023-07-28T03:30:38.185656Z","iopub.status.idle":"2023-07-28T03:30:38.202703Z","shell.execute_reply":"2023-07-28T03:30:38.201655Z"},"papermill":{"duration":0.032122,"end_time":"2023-07-28T03:30:38.205225","exception":false,"start_time":"2023-07-28T03:30:38.173103","status":"completed"},"tags":[]},"outputs":[],"source":["if not SUBMIT:\n","# Get data from csv.\n","    data_dict = {}\n","    if SUBMIT:\n","        sample_submission_file = f'{src}/sample_submission.csv'\n","    else:\n","        sample_submission_file = f'{src}/train/train_labels.csv'\n","\n","    sample_df = pd.read_csv(sample_submission_file)\n","\n","\n","    for i,j in sample_df.iterrows():\n","        dataset = j[\"dataset\"]\n","        scene = j[\"scene\"]\n","\n","        data_dict.setdefault(dataset,{}).setdefault(scene,[]).append(j[\"image_path\"])\n","\n","    if False:\n","        for dataset,value in data_dict.items():\n","            for scene,image_dir in value.items():\n","                scene_dir = f\"{src}/{SET}/{dataset}/{scene}\"\n","                if \"images_full\" in os.listdir(scene_dir):\n","\n","                    image_dir = f\"{scene_dir}/images_full\"\n","                    for image in os.listdir(image_dir):\n","                        data_dict[dataset][scene].append(os.path.join(image_dir,image))\n","\n","    if not SUBMIT:\n","        a = data_dict[\"heritage\"].pop(\"dioscuri\")\n","#         b = data_dict.pop(\"haiper\")\n","#         b = data_dict.pop(\"heritage\")\n","    for dataset in data_dict:\n","        for scene in data_dict[dataset]:\n","            print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n","    \n","        \n","    rotation_thresholds_degrees_dict = {\n","        **{('haiper', scene): np.linspace(0.2, 10, 10) for scene in ['bike', 'chairs', 'fountain']},\n","        **{('heritage', scene): np.linspace(0.2, 10, 10) for scene in ['cyprus', 'dioscuri']},\n","        **{('heritage', 'wall'): np.linspace(0.2, 10, 10)},\n","        **{('urban', 'kyiv-puppet-theater'): np.linspace(0.2, 10, 10)},\n","    }\n","\n","    translation_thresholds_meters_dict = {\n","        **{('haiper', scene): np.geomspace(0.05, 1, 10) for scene in ['bike', 'chairs', 'fountain']},\n","        **{('heritage', scene): np.geomspace(0.05, 1, 10) for scene in ['cyprus', 'dioscuri']},\n","        **{('heritage', 'wall'): np.geomspace(0.05, 1, 10)},\n","        **{('urban', 'kyiv-puppet-theater'): np.geomspace(0.5, 1, 10)},\n","    }\n","    \n","    df = pd.read_csv(\"/kaggle/input/image-matching-challenge-2023/train/train_labels.csv\")\n","    df = df[['image_path','dataset', 'scene',  'rotation_matrix',\n","           'translation_vector']]\n","    df = df[df[\"scene\"]!=\"dioscuri\"]\n","#     df = df[df[\"dataset\"]!=\"heritage\"]\n","#     df = df[df[\"dataset\"]!=\"haiper\"]\n","    df.to_csv(\"train.csv\",index=False)\n","    \n","    "]},{"cell_type":"code","execution_count":27,"id":"3b7b4ffd","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:38.231064Z","iopub.status.busy":"2023-07-28T03:30:38.229939Z","iopub.status.idle":"2023-07-28T03:30:38.239852Z","shell.execute_reply":"2023-07-28T03:30:38.238837Z"},"papermill":{"duration":0.025653,"end_time":"2023-07-28T03:30:38.242248","exception":false,"start_time":"2023-07-28T03:30:38.216595","status":"completed"},"tags":[]},"outputs":[],"source":["if not SUBMIT:\n","    expr = {}\n","    import itertools\n","    descriptors = [\"HARDNET\"]\n","    feats = [2096]\n","    sizes = [800]\n","    composite_status = [True]\n","    upright_ = [False]\n","    \n","    for DESCRIPTOR,NUM_FEATS,RESIZE_SMALL_EDGE_TO,COMPOSITE_DESCRIPTOR,UPRIGHT in itertools.product(*[descriptors,feats,sizes,composite_status,upright_]):\n","        out_results = {}\n","        timings = {\"shortlisting\":[],\n","                       \"feature_detection\": [],\n","                       \"feature_matching\":[],\n","                       \"RANSAC\": [],\n","                       \"Reconstruction\": []}\n","        t = time()\n","        create_submission(data_dict)\n","        t = time()-t\n","        expr[(DESCRIPTOR,NUM_FEATS,RESIZE_SMALL_EDGE_TO,f\"COMPOSITE_DESCRIPTOR_{str(COMPOSITE_DESCRIPTOR)}\",f\"UPRIGHT_{str(UPRIGHT)}\")] = (eval_submission(submission_csv_path='submission.csv',\n","               ground_truth_csv_path='train.csv',\n","               rotation_thresholds_degrees_dict=rotation_thresholds_degrees_dict,\n","               translation_thresholds_meters_dict=translation_thresholds_meters_dict,\n","               verbose=True),t)\n","        \n","    print(expr)"]},{"cell_type":"code","execution_count":28,"id":"ccf0ca41","metadata":{"execution":{"iopub.execute_input":"2023-07-28T03:30:38.267963Z","iopub.status.busy":"2023-07-28T03:30:38.266887Z","iopub.status.idle":"2023-07-28T03:30:38.272383Z","shell.execute_reply":"2023-07-28T03:30:38.271386Z"},"papermill":{"duration":0.020881,"end_time":"2023-07-28T03:30:38.274741","exception":false,"start_time":"2023-07-28T03:30:38.25386","status":"completed"},"tags":[]},"outputs":[],"source":["# {('HARDNET', 4096, 800, 'COMPOSITE_DESCRIPTOR_False', 'UPRIGHT_True'): (0.21076925, 149.39732480049133), ('HARDNET', 4096, 800, 'COMPOSITE_DESCRIPTOR_False', 'UPRIGHT_False'): (0.32892305, 146.10149264335632)}"]},{"cell_type":"code","execution_count":null,"id":"af33b02c","metadata":{"papermill":{"duration":0.010915,"end_time":"2023-07-28T03:30:38.297267","exception":false,"start_time":"2023-07-28T03:30:38.286352","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"3b24e100","metadata":{"papermill":{"duration":0.010805,"end_time":"2023-07-28T03:30:38.319807","exception":false,"start_time":"2023-07-28T03:30:38.309002","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"5d1fba9c","metadata":{"papermill":{"duration":0.010936,"end_time":"2023-07-28T03:30:38.342066","exception":false,"start_time":"2023-07-28T03:30:38.33113","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ee3013fb","metadata":{"papermill":{"duration":0.010874,"end_time":"2023-07-28T03:30:38.364193","exception":false,"start_time":"2023-07-28T03:30:38.353319","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"33b67e0d","metadata":{"papermill":{"duration":0.011073,"end_time":"2023-07-28T03:30:38.386539","exception":false,"start_time":"2023-07-28T03:30:38.375466","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":25.005943,"end_time":"2023-07-28T03:30:40.882951","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-07-28T03:30:15.877008","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}